{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479bfceb",
   "metadata": {},
   "source": [
    "# Analysis Target Features\n",
    "\n",
    "These features are the ones to be analyzed according to our reserach questions\n",
    "\n",
    "## DOMINIO \n",
    "\n",
    "Classifies the interviewee's area as urban, rural or urban-complement\n",
    "\n",
    "## EDAD\n",
    "\n",
    "Age of the inteviewee\n",
    "\n",
    "## P3_8\n",
    "\n",
    "Interviewee marital status\n",
    "* A1 = Married or living with partner\n",
    "* A2 = Married with a temporarily absent partner\n",
    "* B1 = Divorced\n",
    "* B2 = Widowed\n",
    "* C1 = Single\n",
    "* C2 = Has never had a partner\n",
    "\n",
    "## P10_2\n",
    "\n",
    "Asks if the interviwee has had a pregnancy in the last 5 years\n",
    "\n",
    "## P10_7\n",
    "\n",
    "Asks which hospital the interviewee attended during her pregnancy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a4a6f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4a4a6f2",
    "outputId": "2d3e552a-8ed3-458b-f0e6-9f61cbe8bedf"
   },
   "outputs": [],
   "source": [
    "# Initial imports.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sql\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, ConfusionMatrixDisplay\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e14855",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87e14855",
    "outputId": "0f8023b9-0a54-488d-d8cd-fd62cac5778f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter database password········\n"
     ]
    }
   ],
   "source": [
    "# Ask for the RDS database pasword\n",
    "password = getpass('Enter database password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04b8263",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f04b8263",
    "outputId": "a7ccc30c-93df-430a-ebe4-03a4f78081db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TVIV', 'TSDem', 'TB_SEC_III', 'TB_SEC_IV', 'TB_SEC_X', 'obstetric_violence']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create engine to connect to PostgreSQL RDS database\n",
    "engine = sql.create_engine(f'postgresql://postgres:{password}@obstetric-violence.clstnlifxcx7.us-west-2.rds.amazonaws.com:5432/ENDIREH_2021')\n",
    "\n",
    "# Get list of table names\n",
    "sql.inspect(engine).get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad74d14f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "ad74d14f",
    "outputId": "0b8046d3-44b9-45f4-86dc-b72b9fac4fd8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_PER</th>\n",
       "      <th>ID_VIV</th>\n",
       "      <th>UPM</th>\n",
       "      <th>VIV_SEL</th>\n",
       "      <th>HOGAR</th>\n",
       "      <th>N_REN</th>\n",
       "      <th>CVE_ENT</th>\n",
       "      <th>NOM_ENT</th>\n",
       "      <th>CVE_MUN</th>\n",
       "      <th>NOM_MUN</th>\n",
       "      <th>...</th>\n",
       "      <th>P10_8_6</th>\n",
       "      <th>P10_8_7</th>\n",
       "      <th>P10_8_8</th>\n",
       "      <th>P10_8_9</th>\n",
       "      <th>P10_8_10</th>\n",
       "      <th>P10_8_11</th>\n",
       "      <th>P10_8_12</th>\n",
       "      <th>P10_8_13</th>\n",
       "      <th>P10_8_14</th>\n",
       "      <th>P10_8_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0100128.05.1.02</td>\n",
       "      <td>100128.05</td>\n",
       "      <td>100128</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101482.03.1.03</td>\n",
       "      <td>101482.03</td>\n",
       "      <td>101482</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101631.04.1.01</td>\n",
       "      <td>101631.04</td>\n",
       "      <td>101631</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101876.04.1.02</td>\n",
       "      <td>101876.04</td>\n",
       "      <td>101876</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0102096.02.1.02</td>\n",
       "      <td>102096.02</td>\n",
       "      <td>102096</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>5</td>\n",
       "      <td>JESÚS MARÍA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID_PER     ID_VIV     UPM  VIV_SEL  HOGAR  N_REN  CVE_ENT  \\\n",
       "0  0100128.05.1.02  100128.05  100128        5      1      2        1   \n",
       "1  0101482.03.1.03  101482.03  101482        3      1      3        1   \n",
       "2  0101631.04.1.01  101631.04  101631        4      1      1        1   \n",
       "3  0101876.04.1.02  101876.04  101876        4      1      2        1   \n",
       "4  0102096.02.1.02  102096.02  102096        2      1      2        1   \n",
       "\n",
       "          NOM_ENT  CVE_MUN         NOM_MUN  ...  P10_8_6  P10_8_7 P10_8_8  \\\n",
       "0  AGUASCALIENTES        1  AGUASCALIENTES  ...      NaN      NaN     NaN   \n",
       "1  AGUASCALIENTES        1  AGUASCALIENTES  ...      NaN      NaN     NaN   \n",
       "2  AGUASCALIENTES        1  AGUASCALIENTES  ...      NaN      NaN     NaN   \n",
       "3  AGUASCALIENTES        1  AGUASCALIENTES  ...      2.0      2.0     2.0   \n",
       "4  AGUASCALIENTES        5     JESÚS MARÍA  ...      NaN      NaN     NaN   \n",
       "\n",
       "   P10_8_9  P10_8_10  P10_8_11 P10_8_12  P10_8_13  P10_8_14  P10_8_15  \n",
       "0      NaN       NaN       NaN      NaN       NaN       NaN       NaN  \n",
       "1      NaN       NaN       NaN      NaN       NaN       NaN       NaN  \n",
       "2      NaN       NaN       NaN      NaN       NaN       NaN       NaN  \n",
       "3      2.0       2.0       2.0      1.0       1.0       1.0       NaN  \n",
       "4      NaN       NaN       NaN      NaN       NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 170 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the obstetric_violence table and show the results\n",
    "RDS_df = pd.read_sql_table('obstetric_violence', con=engine)\n",
    "RDS_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82ff068",
   "metadata": {
    "id": "d82ff068"
   },
   "outputs": [],
   "source": [
    "# Creating a copy of the database to choose the features we will use to analyse\n",
    "df_copy = RDS_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f27f8",
   "metadata": {},
   "source": [
    "The features removed in the following block belong to one of the following categories:\n",
    "\n",
    "    1. Interviewee identification numbers (ID_VIV, ID_PER, UPM, VIV_SEL, HOGAR, N_REN)\n",
    "    \n",
    "    2. Interview structure control (UPM_DIS, ESTRATO, REN_MUJ_EL, REN_INF_AD, N_REN_ESP)\n",
    "    \n",
    "    3. Are redundant (CVE_ENT, CVE_MUN, SEXO, COD_M15, CODIGO, T_INSTRUM, GRA)\n",
    "    \n",
    "    4. Are related to the survey sampling strategy (FAC_VIV, FAC_MUJ)\n",
    "    \n",
    "    5. Are not relevant to the analysis (NOM_MUN, P4_4_CVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0249a12",
   "metadata": {
    "id": "f0249a12"
   },
   "outputs": [],
   "source": [
    "# Remove columns that had data that wasn't usefull like ids, sampling information and table structure\n",
    "df_copy = df_copy.drop(columns=['ID_VIV', 'ID_PER' ,'UPM', \n",
    "                                'VIV_SEL', 'HOGAR', 'N_REN', \n",
    "                                'EST_DIS', 'UPM_DIS', 'ESTRATO', \n",
    "                                'NOMBRE', 'SEXO', 'COD_M15', \n",
    "                                'CODIGO', 'REN_MUJ_EL', 'REN_INF_AD', \n",
    "                                'N_REN_ESP','T_INSTRUM', 'FAC_VIV', \n",
    "                                'FAC_MUJ', 'PAREN', 'GRA', \n",
    "                                'NOM_MUN', 'P4_4_CVE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654be9f",
   "metadata": {},
   "source": [
    "In the following block, P10_2 is used to remove certain entries from the dataset. \n",
    "\n",
    "This question asks the interviewee if they've had a pregnancy in the last 5 years, which is the target population for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a820335c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "a820335c",
    "outputId": "79e3d209-da14-42c0-b6b4-3f03c90b5a5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CVE_ENT</th>\n",
       "      <th>NOM_ENT</th>\n",
       "      <th>CVE_MUN</th>\n",
       "      <th>COD_RES</th>\n",
       "      <th>DOMINIO</th>\n",
       "      <th>EDAD</th>\n",
       "      <th>NIV</th>\n",
       "      <th>P1_1</th>\n",
       "      <th>P1_2</th>\n",
       "      <th>P1_2_A</th>\n",
       "      <th>...</th>\n",
       "      <th>P10_8_6</th>\n",
       "      <th>P10_8_7</th>\n",
       "      <th>P10_8_8</th>\n",
       "      <th>P10_8_9</th>\n",
       "      <th>P10_8_10</th>\n",
       "      <th>P10_8_11</th>\n",
       "      <th>P10_8_12</th>\n",
       "      <th>P10_8_13</th>\n",
       "      <th>P10_8_14</th>\n",
       "      <th>P10_8_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>U</td>\n",
       "      <td>45</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AGUASCALIENTES</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>31</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BAJA CALIFORNIA</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>U</td>\n",
       "      <td>27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>BAJA CALIFORNIA</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>U</td>\n",
       "      <td>25</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>COLIMA</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>U</td>\n",
       "      <td>30</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CVE_ENT          NOM_ENT  CVE_MUN  COD_RES DOMINIO  EDAD   NIV  P1_1  P1_2  \\\n",
       "0        1   AGUASCALIENTES        1        1       U    45  11.0     3     3   \n",
       "1        1   AGUASCALIENTES       11        1       R    31   4.0     3     2   \n",
       "2        2  BAJA CALIFORNIA        4        1       U    27   4.0     3     1   \n",
       "3        2  BAJA CALIFORNIA        4        1       U    25  10.0     3     1   \n",
       "4        6           COLIMA       10        1       U    30   9.0     3     2   \n",
       "\n",
       "   P1_2_A  ...  P10_8_6  P10_8_7  P10_8_8  P10_8_9  P10_8_10  P10_8_11  \\\n",
       "0       5  ...      2.0      2.0      2.0      2.0       2.0       2.0   \n",
       "1       3  ...      2.0      2.0      2.0      2.0       2.0       2.0   \n",
       "2       1  ...      2.0      2.0      2.0      2.0       2.0       2.0   \n",
       "3       3  ...      2.0      1.0      2.0      2.0       2.0       2.0   \n",
       "4       4  ...      NaN      NaN      NaN      NaN       NaN       NaN   \n",
       "\n",
       "   P10_8_12  P10_8_13  P10_8_14  P10_8_15  \n",
       "0       1.0       1.0       1.0       NaN  \n",
       "1       1.0       1.0       1.0       NaN  \n",
       "2       2.0       NaN       NaN       NaN  \n",
       "3       1.0       1.0       1.0       NaN  \n",
       "4       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing women that did not had a pregnancy on the last 5 years\n",
    "df_copy = df_copy[df_copy.P10_2 == 1.0].reset_index(drop=True)\n",
    "df_copy.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0ca91",
   "metadata": {},
   "source": [
    "The questions going from P10_8_1 to P10_8_11 were selected because they belong to obstetric violence situations. \n",
    "\n",
    "Questions before section P10_8_1 serve to get a clearer image of the interviewee's financial and marital information, as well as information on what kind of hospital they were receveived.\n",
    "\n",
    "Questions P10_8_12 to P10_8_15 had very few samples to be analyzed, P10_8_15 had less than 1000 answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39521320",
   "metadata": {
    "id": "39521320"
   },
   "outputs": [],
   "source": [
    "#List of each target question we chose \n",
    "targets = ['P10_8_1','P10_8_2','P10_8_3',\n",
    "          'P10_8_4','P10_8_5','P10_8_6',\n",
    "          'P10_8_7','P10_8_8','P10_8_9',\n",
    "          'P10_8_10','P10_8_11']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39145be",
   "metadata": {},
   "source": [
    "The DataFrame_X_y_split function serves the following purposes:\n",
    "\n",
    "    1. Set the appropriate dtype to each of the columns according to the information stored by each, be it categorical or numerical\n",
    "    \n",
    "    2. Clean the dataset from mixed information, for example, in the income columns the number 999999 represents that the interviewee did not answer the question  \n",
    "    \n",
    "        1. This value when analyzed is also an outlier when compared with the average values of said column\n",
    "        \n",
    "    3. Fill the empty values with 'b' or 0 depending on the feature's dtype\n",
    "    \n",
    "    4. Remove the target quetions (y) from the X dataset\n",
    "    \n",
    "    5. Clean the string values for question P4_4 since it contains the profession of the interviewee\n",
    "    \n",
    "        1. The chosen approach was to pick the first word of the written answer because in most cases this hold the job information\n",
    "        \n",
    "        2. Cases where the first word is an article are labeled as other due to time constraints.\n",
    "        \n",
    "    6. Finally, the X dataset for each target question is created by removing all rows where the target question is left blank, only the yes/no values are kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac2cbbbe",
   "metadata": {
    "id": "ac2cbbbe"
   },
   "outputs": [],
   "source": [
    "# Function to create a dataset for each target question and store it in a dictionary\n",
    "def DataFrame_X_y_split(source_df, df_X_y_dict = {}):\n",
    "    # Create target question list\n",
    "    question_list = ['P10_8_1','P10_8_2','P10_8_3',\n",
    "                     'P10_8_4','P10_8_5','P10_8_6',\n",
    "                     'P10_8_7','P10_8_8','P10_8_9',\n",
    "                     'P10_8_10','P10_8_11']\n",
    "    \n",
    "    # Create the chosen feature list\n",
    "    feature_list = ['DOMINIO', 'EDAD', \n",
    "                    'P3_8', 'P10_2', 'P10_7']\n",
    "    \n",
    "    # Create a copy of the dataframe to avoid making changes in the original\n",
    "    df = source_df.copy()\n",
    "    \n",
    "    # Grab only the target features\n",
    "    df = df[feature_list + question_list]\n",
    "    \n",
    "    # Chose only the target features from the dataset and set dtype as string\n",
    "    df[feature_list] = df[feature_list].fillna('b').astype(str)\n",
    "    df['EDAD'] = df['EDAD'].astype(float).fillna(0).astype(int)\n",
    "\n",
    "    # Enconde the categorical features\n",
    "    encode_df = pd.get_dummies(df, dtype=float)\n",
    "    \n",
    "    # Create the dataset for each question\n",
    "    for target in question_list:\n",
    "        # Drop the rows where the target answers are blank\n",
    "        df_X = encode_df.loc[(encode_df[target] == 1) | (encode_df[target] == 2)].drop(columns=question_list)\n",
    "        df_y = encode_df.loc[(encode_df[target] == 1) | (encode_df[target] == 2),[target]]\n",
    "        # Create nested dictionary for the target question\n",
    "        df_X_y_dict[target] = {}\n",
    "        # Store the X and y datasets that will be used with the random forest model for the key question\n",
    "        df_X_y_dict[target]['X'] = df_X\n",
    "        df_X_y_dict[target]['y'] = df_y\n",
    "\n",
    "    return df_X_y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c1c2e7d",
   "metadata": {
    "id": "6c1c2e7d"
   },
   "outputs": [],
   "source": [
    "# Datasets for each target question\n",
    "dataset_dictionary = DataFrame_X_y_split(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ULCv6TgHbimk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "ULCv6TgHbimk",
    "outputId": "65c3982d-c5e4-444b-806f-83aecfa95fdd",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EDAD</th>\n",
       "      <th>DOMINIO_C</th>\n",
       "      <th>DOMINIO_R</th>\n",
       "      <th>DOMINIO_U</th>\n",
       "      <th>P3_8_A1</th>\n",
       "      <th>P3_8_A2</th>\n",
       "      <th>P3_8_B1</th>\n",
       "      <th>P3_8_B2</th>\n",
       "      <th>P3_8_C1</th>\n",
       "      <th>P3_8_C2</th>\n",
       "      <th>...</th>\n",
       "      <th>P10_7_10.0</th>\n",
       "      <th>P10_7_2.0</th>\n",
       "      <th>P10_7_3.0</th>\n",
       "      <th>P10_7_4.0</th>\n",
       "      <th>P10_7_5.0</th>\n",
       "      <th>P10_7_6.0</th>\n",
       "      <th>P10_7_7.0</th>\n",
       "      <th>P10_7_8.0</th>\n",
       "      <th>P10_7_9.0</th>\n",
       "      <th>P10_7_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20942</th>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20943</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20944</th>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20945</th>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20946</th>\n",
       "      <td>31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19322 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       EDAD  DOMINIO_C  DOMINIO_R  DOMINIO_U  P3_8_A1  P3_8_A2  P3_8_B1  \\\n",
       "0        45        0.0        0.0        1.0      1.0      0.0      0.0   \n",
       "1        31        0.0        1.0        0.0      1.0      0.0      0.0   \n",
       "2        27        0.0        0.0        1.0      1.0      0.0      0.0   \n",
       "3        25        0.0        0.0        1.0      1.0      0.0      0.0   \n",
       "6        29        0.0        1.0        0.0      1.0      0.0      0.0   \n",
       "...     ...        ...        ...        ...      ...      ...      ...   \n",
       "20942    25        0.0        0.0        1.0      1.0      0.0      0.0   \n",
       "20943    33        0.0        1.0        0.0      1.0      0.0      0.0   \n",
       "20944    33        1.0        0.0        0.0      1.0      0.0      0.0   \n",
       "20945    35        0.0        0.0        1.0      1.0      0.0      0.0   \n",
       "20946    31        1.0        0.0        0.0      1.0      0.0      0.0   \n",
       "\n",
       "       P3_8_B2  P3_8_C1  P3_8_C2  ...  P10_7_10.0  P10_7_2.0  P10_7_3.0  \\\n",
       "0          0.0      0.0      0.0  ...         0.0        0.0        0.0   \n",
       "1          0.0      0.0      0.0  ...         0.0        0.0        0.0   \n",
       "2          0.0      0.0      0.0  ...         0.0        0.0        0.0   \n",
       "3          0.0      0.0      0.0  ...         0.0        0.0        0.0   \n",
       "6          0.0      0.0      0.0  ...         0.0        0.0        0.0   \n",
       "...        ...      ...      ...  ...         ...        ...        ...   \n",
       "20942      0.0      0.0      0.0  ...         0.0        1.0        0.0   \n",
       "20943      0.0      0.0      0.0  ...         0.0        0.0        0.0   \n",
       "20944      0.0      0.0      0.0  ...         0.0        0.0        0.0   \n",
       "20945      0.0      0.0      0.0  ...         0.0        0.0        0.0   \n",
       "20946      0.0      0.0      0.0  ...         0.0        0.0        0.0   \n",
       "\n",
       "       P10_7_4.0  P10_7_5.0  P10_7_6.0  P10_7_7.0  P10_7_8.0  P10_7_9.0  \\\n",
       "0            0.0        0.0        1.0        0.0        0.0        0.0   \n",
       "1            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "2            0.0        0.0        1.0        0.0        0.0        0.0   \n",
       "3            0.0        0.0        1.0        0.0        0.0        0.0   \n",
       "6            0.0        1.0        0.0        0.0        0.0        0.0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "20942        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "20943        0.0        0.0        1.0        0.0        0.0        0.0   \n",
       "20944        0.0        0.0        0.0        1.0        0.0        0.0   \n",
       "20945        0.0        0.0        0.0        1.0        0.0        0.0   \n",
       "20946        0.0        1.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "       P10_7_b  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "6          0.0  \n",
       "...        ...  \n",
       "20942      0.0  \n",
       "20943      0.0  \n",
       "20944      0.0  \n",
       "20945      0.0  \n",
       "20946      0.0  \n",
       "\n",
       "[19322 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test to confirm if the information was correctly stored in the dictionary\n",
    "dataset_dictionary['P10_8_11']['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7ca94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','tanh','swish'])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=50,\n",
    "        max_value=200), activation=activation, input_dim=22))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=50,\n",
    "            max_value=200),activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc779e4",
   "metadata": {},
   "source": [
    "The NN_Classifier function serves the following purposes:\n",
    "\n",
    "    1. The weights for each class are calculated using the binary count for the y dataset\n",
    "    \n",
    "    2. The weights are added in a dictionary which is then passed as an argument for the neural network\n",
    "    \n",
    "    3. The X y datasets are split on a 75/25 ratio for training and testing\n",
    "\n",
    "        1. The y dataset is stratified since the dataset is imbalanced (around 90% answered no and 10% answered yes)\n",
    "        \n",
    "    4. The X datasets are scaled using a standard scaler\n",
    "    \n",
    "    5. The neural network optimizer from keras tuner is used to get the best hyperparameters for each model\n",
    "    \n",
    "    6. The best model from the keras tuner search is stored and trained further to then make a prediction\n",
    "    \n",
    "    7. The results from the prediction, which are a probability of being the value 1, are converted to a binary result using an adjusted threshold\n",
    "    \n",
    "        1. This threshold is calculated using the gmeans method\n",
    "        \n",
    "    8. Finally, the model, performance metrics and results are stored inside a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "G-lwg8eXZhzv",
   "metadata": {
    "id": "G-lwg8eXZhzv"
   },
   "outputs": [],
   "source": [
    "def NN_Classifier(key, dict_X, dict_y, NN_Results = {}):\n",
    "\n",
    "    # Create a copy of the X and y datasets to prevent modifications in the original dataset\n",
    "    X = dict_X.copy()\n",
    "    y = dict_y.copy()\n",
    "\n",
    "    # Change the y labels from 1 and 2 to 0 and 1 respectively\n",
    "    y.loc[y[key] == 1,key] = 0\n",
    "    y.loc[y[key] == 2,key] = 1\n",
    "\n",
    "    # Calculate the count of 0s and 1s\n",
    "    pos, neg = np.bincount(y[key])\n",
    "\n",
    "    # Calculate the count of values in y\n",
    "    total = neg + pos\n",
    "\n",
    "    # Calculate the class weight\n",
    "    weight_for_0 = (24 / pos) * (total)\n",
    "    weight_for_1 = (1 / neg) * (total)\n",
    "\n",
    "    # Create the class weight dictionary\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "    # Grab the y information from the target dataset\n",
    "    y = y.astype(int).values\n",
    "\n",
    "    # Create the train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values, y, random_state=18, stratify=y)\n",
    "\n",
    "    # Create a scaler instance\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Train the standard scaler using the X_train data\n",
    "    X_scaler = scaler.fit(X_train)\n",
    "\n",
    "    # Scale the X training data\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "\n",
    "    # Scale the X test data\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # Import the kerastuner library\n",
    "    tuner = kt.Hyperband(\n",
    "        create_model,\n",
    "        objective=\"val_loss\",\n",
    "        max_epochs=30,\n",
    "        factor=3,\n",
    "        hyperband_iterations=2,\n",
    "        overwrite=True,\n",
    "        directory=\"optimizer_runs\",\n",
    "        project_name=f'{key}_model_optimizer')\n",
    "\n",
    "     # Run the kerastuner search for best hyperparameters\n",
    "    tuner.search(X_train_scaled,y_train,epochs=30,validation_data=(X_test_scaled,y_test), class_weight=class_weight)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_hps = tuner.get_best_hyperparameters(1)\n",
    "    \n",
    "    # Build neural network\n",
    "    nn = create_model(best_hps[0])\n",
    "    \n",
    "    # Train the model \n",
    "    fit_model = nn.fit(X_train_scaled,y_train,epochs=580,class_weight=class_weight)\n",
    "\n",
    "    # Predict the results for the target question\n",
    "    predictions = nn.predict(X_test_scaled).ravel()\n",
    "\n",
    "    # calculate roc curves\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # REFERENCE https://towardsdatascience.com/optimal-threshold-for-imbalanced-classification-5884e870c293\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    # Calculate the G-mean\n",
    "    gmean = np.sqrt(tpr * (1 - fpr))\n",
    "\n",
    "    # Find the optimal threshold\n",
    "    index = np.argmax(gmean)\n",
    "    thresholdOpt = round(thresholds[index], ndigits = 4)\n",
    "    gmeanOpt = round(gmean[index], ndigits = 4)\n",
    "    fprOpt = round(fpr[index], ndigits = 4)\n",
    "    tprOpt = round(tpr[index], ndigits = 4)\n",
    "    print('Best Threshold: {} with G-Mean: {}'.format(thresholdOpt, gmeanOpt))\n",
    "    print('FPR: {}, TPR: {}'.format(fprOpt, tprOpt))\n",
    "    \n",
    "    # Convert predictions to 0 or 1 according to the optimal threshold\n",
    "    threshold = thresholdOpt\n",
    "\n",
    "    # Label predictions using the threshold\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "\n",
    "    # Calculating the confusion matrix.\n",
    "    cm = confusion_matrix(y_test, binary_predictions)\n",
    "\n",
    "    # Evaluate the model using the test data\n",
    "    model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "    pyplot.plot(fpr, tpr, marker='.', label='Neural Network')\n",
    "    pyplot.plot(fprOpt, tprOpt, marker='*', label='Optimal Value')\n",
    "\n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.legend()\n",
    "    \n",
    "    # save the roc curve\n",
    "    pyplot.savefig(f'plots/{key}_roc_plot.png')\n",
    "    \n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    \n",
    "    # display confusion matrix plot\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    pyplot.savefig(f'plots/{key}_cm.png')\n",
    "    pyplot.show()\n",
    "    \n",
    "    # Save the neural network model\n",
    "    nn.save(f'Results/{key}_model.h5')\n",
    "    \n",
    "    # Store the results results\n",
    "    NN_Results = {}\n",
    "    NN_Results['Threshold'] = thresholdOpt\n",
    "    NN_Results['Predictions'] = binary_predictions\n",
    "    NN_Results[\"Confusion Matrix\"] = cm\n",
    "    NN_Results[\"Accuracy Score\"] = model_accuracy\n",
    "    NN_Results[\"Classification Report\"] = classification_report(y_test, binary_predictions, target_names=['Class 1', 'Class 2'])    \n",
    "    return NN_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "JJDJ1_9Hjs_S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JJDJ1_9Hjs_S",
    "outputId": "a92270a2-93e5-4906-d4e9-7967f973e491",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 180 Complete [00h 00m 20s]\n",
      "val_loss: 2.7086706161499023\n",
      "\n",
      "Best val_loss So Far: 2.2359161376953125\n",
      "Total elapsed time: 00h 15m 33s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/580\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 5.0947 - accuracy: 0.0554\n",
      "Epoch 2/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.3873 - accuracy: 0.0304\n",
      "Epoch 3/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.2137 - accuracy: 0.0302\n",
      "Epoch 4/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1541 - accuracy: 0.0416\n",
      "Epoch 5/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.2482 - accuracy: 0.0422\n",
      "Epoch 6/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1312 - accuracy: 0.0409\n",
      "Epoch 7/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1049 - accuracy: 0.0418\n",
      "Epoch 8/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0926 - accuracy: 0.0431\n",
      "Epoch 9/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0672 - accuracy: 0.0495\n",
      "Epoch 10/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.3151 - accuracy: 0.0458\n",
      "Epoch 11/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1150 - accuracy: 0.0505\n",
      "Epoch 12/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.2194 - accuracy: 0.0411\n",
      "Epoch 13/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1342 - accuracy: 0.0301\n",
      "Epoch 14/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0570 - accuracy: 0.0518\n",
      "Epoch 15/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0791 - accuracy: 0.0575\n",
      "Epoch 16/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0642 - accuracy: 0.0634\n",
      "Epoch 17/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0131 - accuracy: 0.0660\n",
      "Epoch 18/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0874 - accuracy: 0.0659\n",
      "Epoch 19/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0097 - accuracy: 0.0683\n",
      "Epoch 20/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0277 - accuracy: 0.0711\n",
      "Epoch 21/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0778 - accuracy: 0.0780\n",
      "Epoch 22/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0982 - accuracy: 0.0570\n",
      "Epoch 23/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.2323 - accuracy: 0.0708\n",
      "Epoch 24/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0149 - accuracy: 0.0707\n",
      "Epoch 25/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0084 - accuracy: 0.0747\n",
      "Epoch 26/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0831 - accuracy: 0.0709\n",
      "Epoch 27/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1775 - accuracy: 0.0629\n",
      "Epoch 28/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9627 - accuracy: 0.0745\n",
      "Epoch 29/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9878 - accuracy: 0.0716\n",
      "Epoch 30/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9345 - accuracy: 0.0744\n",
      "Epoch 31/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0055 - accuracy: 0.0747\n",
      "Epoch 32/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9620 - accuracy: 0.0818\n",
      "Epoch 33/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0516 - accuracy: 0.0664\n",
      "Epoch 34/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9829 - accuracy: 0.0636\n",
      "Epoch 35/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9653 - accuracy: 0.0736\n",
      "Epoch 36/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9166 - accuracy: 0.0792\n",
      "Epoch 37/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9277 - accuracy: 0.0858\n",
      "Epoch 38/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8821 - accuracy: 0.0890\n",
      "Epoch 39/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9004 - accuracy: 0.0925\n",
      "Epoch 40/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9080 - accuracy: 0.0927\n",
      "Epoch 41/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8976 - accuracy: 0.0899\n",
      "Epoch 42/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8803 - accuracy: 0.0936\n",
      "Epoch 43/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8513 - accuracy: 0.0954\n",
      "Epoch 44/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0155 - accuracy: 0.0901\n",
      "Epoch 45/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8671 - accuracy: 0.0874\n",
      "Epoch 46/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9304 - accuracy: 0.0905\n",
      "Epoch 47/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8872 - accuracy: 0.0878\n",
      "Epoch 48/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8352 - accuracy: 0.0979\n",
      "Epoch 49/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8313 - accuracy: 0.1005\n",
      "Epoch 50/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8523 - accuracy: 0.1013\n",
      "Epoch 51/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9379 - accuracy: 0.0903\n",
      "Epoch 52/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8307 - accuracy: 0.0930\n",
      "Epoch 53/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0417 - accuracy: 0.0925\n",
      "Epoch 54/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8077 - accuracy: 0.0966\n",
      "Epoch 55/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8324 - accuracy: 0.0978\n",
      "Epoch 56/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8416 - accuracy: 0.0950\n",
      "Epoch 57/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8494 - accuracy: 0.0921\n",
      "Epoch 58/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9193 - accuracy: 0.1026\n",
      "Epoch 59/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7893 - accuracy: 0.1039\n",
      "Epoch 60/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7820 - accuracy: 0.1032\n",
      "Epoch 61/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7807 - accuracy: 0.1061\n",
      "Epoch 62/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0637 - accuracy: 0.1028\n",
      "Epoch 63/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7625 - accuracy: 0.1025\n",
      "Epoch 64/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7345 - accuracy: 0.1092\n",
      "Epoch 65/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7432 - accuracy: 0.1068\n",
      "Epoch 66/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8366 - accuracy: 0.1039\n",
      "Epoch 67/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8229 - accuracy: 0.1079\n",
      "Epoch 68/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8070 - accuracy: 0.1019\n",
      "Epoch 69/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7121 - accuracy: 0.1127\n",
      "Epoch 70/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8098 - accuracy: 0.1039\n",
      "Epoch 71/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9179 - accuracy: 0.1092\n",
      "Epoch 72/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7603 - accuracy: 0.1140\n",
      "Epoch 73/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8164 - accuracy: 0.1283\n",
      "Epoch 74/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8405 - accuracy: 0.1257\n",
      "Epoch 75/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1546 - accuracy: 0.1089\n",
      "Epoch 76/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8016 - accuracy: 0.1173\n",
      "Epoch 77/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8009 - accuracy: 0.1267\n",
      "Epoch 78/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7926 - accuracy: 0.1246\n",
      "Epoch 79/580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7094 - accuracy: 0.1376\n",
      "Epoch 80/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.0905 - accuracy: 0.1260\n",
      "Epoch 81/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8205 - accuracy: 0.1150\n",
      "Epoch 82/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7505 - accuracy: 0.1212\n",
      "Epoch 83/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7099 - accuracy: 0.1283\n",
      "Epoch 84/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6888 - accuracy: 0.1362\n",
      "Epoch 85/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7052 - accuracy: 0.1310\n",
      "Epoch 86/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6852 - accuracy: 0.1381\n",
      "Epoch 87/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7819 - accuracy: 0.1454\n",
      "Epoch 88/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.2024 - accuracy: 0.1360\n",
      "Epoch 89/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7950 - accuracy: 0.1436\n",
      "Epoch 90/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6709 - accuracy: 0.1503\n",
      "Epoch 91/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8154 - accuracy: 0.1568\n",
      "Epoch 92/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8983 - accuracy: 0.1249\n",
      "Epoch 93/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7450 - accuracy: 0.1322\n",
      "Epoch 94/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6871 - accuracy: 0.1421\n",
      "Epoch 95/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7923 - accuracy: 0.1485\n",
      "Epoch 96/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7190 - accuracy: 0.1491\n",
      "Epoch 97/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7768 - accuracy: 0.1366\n",
      "Epoch 98/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6765 - accuracy: 0.1517\n",
      "Epoch 99/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6785 - accuracy: 0.1578\n",
      "Epoch 100/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9965 - accuracy: 0.1394\n",
      "Epoch 101/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1462 - accuracy: 0.1426\n",
      "Epoch 102/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7128 - accuracy: 0.1371\n",
      "Epoch 103/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6448 - accuracy: 0.1489\n",
      "Epoch 104/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6096 - accuracy: 0.1560\n",
      "Epoch 105/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6697 - accuracy: 0.1568\n",
      "Epoch 106/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6441 - accuracy: 0.1478\n",
      "Epoch 107/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8712 - accuracy: 0.1473\n",
      "Epoch 108/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.2898 - accuracy: 0.1607\n",
      "Epoch 109/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 5.0079 - accuracy: 0.1427\n",
      "Epoch 110/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8932 - accuracy: 0.1538\n",
      "Epoch 111/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.5068 - accuracy: 0.1614\n",
      "Epoch 112/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1693 - accuracy: 0.1504\n",
      "Epoch 113/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.9590 - accuracy: 0.1448\n",
      "Epoch 114/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7442 - accuracy: 0.1484\n",
      "Epoch 115/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6956 - accuracy: 0.1574\n",
      "Epoch 116/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.7068 - accuracy: 0.1480\n",
      "Epoch 117/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.7454 - accuracy: 0.1520\n",
      "Epoch 118/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.5831 - accuracy: 0.1577\n",
      "Epoch 119/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1329 - accuracy: 0.1603\n",
      "Epoch 120/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.6411 - accuracy: 0.1683\n",
      "Epoch 121/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.5788 - accuracy: 0.1784\n",
      "Epoch 122/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8262 - accuracy: 0.1775\n",
      "Epoch 123/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.2423 - accuracy: 0.1680\n",
      "Epoch 124/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1521 - accuracy: 0.1653\n",
      "Epoch 125/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.7242 - accuracy: 0.1696\n",
      "Epoch 126/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.7280 - accuracy: 0.1613\n",
      "Epoch 127/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 3.8901 - accuracy: 0.1720\n",
      "Epoch 128/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.4521 - accuracy: 0.1562\n",
      "Epoch 129/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 6.4799 - accuracy: 0.1495\n",
      "Epoch 130/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.5144 - accuracy: 0.1429\n",
      "Epoch 131/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 9.1946 - accuracy: 0.1409\n",
      "Epoch 132/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 5.1307 - accuracy: 0.1455\n",
      "Epoch 133/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.5437 - accuracy: 0.1486\n",
      "Epoch 134/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.4679 - accuracy: 0.1365\n",
      "Epoch 135/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 12.1553 - accuracy: 0.1496\n",
      "Epoch 136/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 5.0515 - accuracy: 0.1593\n",
      "Epoch 137/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.2433 - accuracy: 0.1593\n",
      "Epoch 138/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 5.0874 - accuracy: 0.1542\n",
      "Epoch 139/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.1438 - accuracy: 0.1537\n",
      "Epoch 140/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.3633 - accuracy: 0.1471\n",
      "Epoch 141/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.1736 - accuracy: 0.1637\n",
      "Epoch 142/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 6.4422 - accuracy: 0.1666\n",
      "Epoch 143/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.9820 - accuracy: 0.1595\n",
      "Epoch 144/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 11.5530 - accuracy: 0.1501\n",
      "Epoch 145/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 11.4393 - accuracy: 0.1648\n",
      "Epoch 146/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 6.1563 - accuracy: 0.1507\n",
      "Epoch 147/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 4.5264 - accuracy: 0.1595\n",
      "Epoch 148/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 8.3671 - accuracy: 0.1525\n",
      "Epoch 149/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 15.4522 - accuracy: 0.1435\n",
      "Epoch 150/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.9418 - accuracy: 0.1472\n",
      "Epoch 151/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 5.3989 - accuracy: 0.1507\n",
      "Epoch 152/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 12.1756 - accuracy: 0.1253\n",
      "Epoch 153/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.8878 - accuracy: 0.1356\n",
      "Epoch 154/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 6.5855 - accuracy: 0.1370\n",
      "Epoch 155/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 5.6544 - accuracy: 0.1455\n",
      "Epoch 156/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 10.4347 - accuracy: 0.1273\n",
      "Epoch 157/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 8.4981 - accuracy: 0.1386\n",
      "Epoch 158/580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 1ms/step - loss: 20.2459 - accuracy: 0.1240\n",
      "Epoch 159/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 11.3531 - accuracy: 0.1384\n",
      "Epoch 160/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 11.0122 - accuracy: 0.1373\n",
      "Epoch 161/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 9.0923 - accuracy: 0.1441\n",
      "Epoch 162/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.0251 - accuracy: 0.1308\n",
      "Epoch 163/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 16.5624 - accuracy: 0.1275\n",
      "Epoch 164/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 15.2570 - accuracy: 0.1264\n",
      "Epoch 165/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 9.0704 - accuracy: 0.1313\n",
      "Epoch 166/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 6.5449 - accuracy: 0.1544\n",
      "Epoch 167/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 9.1882 - accuracy: 0.1330\n",
      "Epoch 168/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 6.9690 - accuracy: 0.1401\n",
      "Epoch 169/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 5.6494 - accuracy: 0.1428\n",
      "Epoch 170/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 20.1270 - accuracy: 0.1213\n",
      "Epoch 171/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.5899 - accuracy: 0.1250\n",
      "Epoch 172/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.7381 - accuracy: 0.1415\n",
      "Epoch 173/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 5.2283 - accuracy: 0.1518\n",
      "Epoch 174/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 16.0733 - accuracy: 0.1114\n",
      "Epoch 175/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.8690 - accuracy: 0.1428\n",
      "Epoch 176/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 17.2664 - accuracy: 0.1289\n",
      "Epoch 177/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 11.5367 - accuracy: 0.1102\n",
      "Epoch 178/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 12.6562 - accuracy: 0.1131\n",
      "Epoch 179/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 28.3720 - accuracy: 0.1028\n",
      "Epoch 180/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 16.5612 - accuracy: 0.1254\n",
      "Epoch 181/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 18.1580 - accuracy: 0.1201\n",
      "Epoch 182/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.6253 - accuracy: 0.1349\n",
      "Epoch 183/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 19.7203 - accuracy: 0.1123\n",
      "Epoch 184/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 8.5420 - accuracy: 0.1436\n",
      "Epoch 185/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 10.9194 - accuracy: 0.1363\n",
      "Epoch 186/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.4526 - accuracy: 0.1418\n",
      "Epoch 187/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.2611 - accuracy: 0.1056\n",
      "Epoch 188/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 9.8512 - accuracy: 0.1266\n",
      "Epoch 189/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 24.3094 - accuracy: 0.1073\n",
      "Epoch 190/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 15.1889 - accuracy: 0.1195\n",
      "Epoch 191/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 9.2174 - accuracy: 0.1235\n",
      "Epoch 192/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 5.9826 - accuracy: 0.1265\n",
      "Epoch 193/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 21.9852 - accuracy: 0.1122\n",
      "Epoch 194/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 12.2415 - accuracy: 0.1117\n",
      "Epoch 195/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 22.8452 - accuracy: 0.1086\n",
      "Epoch 196/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 15.3266 - accuracy: 0.0993\n",
      "Epoch 197/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 5.9780 - accuracy: 0.1236\n",
      "Epoch 198/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 32.4933 - accuracy: 0.1037\n",
      "Epoch 199/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 11.9903 - accuracy: 0.1148\n",
      "Epoch 200/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 18.0933 - accuracy: 0.0972\n",
      "Epoch 201/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.7601 - accuracy: 0.1215\n",
      "Epoch 202/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 8.4402 - accuracy: 0.1213\n",
      "Epoch 203/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 16.4979 - accuracy: 0.1159\n",
      "Epoch 204/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 45.8904 - accuracy: 0.0928\n",
      "Epoch 205/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 16.5132 - accuracy: 0.1337\n",
      "Epoch 206/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.0136 - accuracy: 0.1253\n",
      "Epoch 207/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 28.7661 - accuracy: 0.1087\n",
      "Epoch 208/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 11.5602 - accuracy: 0.1241\n",
      "Epoch 209/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 9.6805 - accuracy: 0.1260\n",
      "Epoch 210/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 38.5533 - accuracy: 0.1199\n",
      "Epoch 211/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 28.3138 - accuracy: 0.1003\n",
      "Epoch 212/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 18.2125 - accuracy: 0.1210\n",
      "Epoch 213/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 10.2332 - accuracy: 0.1113\n",
      "Epoch 214/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 41.7742 - accuracy: 0.0947\n",
      "Epoch 215/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 17.7532 - accuracy: 0.1123\n",
      "Epoch 216/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 40.5730 - accuracy: 0.0883\n",
      "Epoch 217/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.7106 - accuracy: 0.0965\n",
      "Epoch 218/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 16.3369 - accuracy: 0.1056\n",
      "Epoch 219/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 11.5071 - accuracy: 0.1054\n",
      "Epoch 220/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 54.7911 - accuracy: 0.1000\n",
      "Epoch 221/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 23.9119 - accuracy: 0.0950\n",
      "Epoch 222/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 31.3652 - accuracy: 0.1086\n",
      "Epoch 223/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 13.8454 - accuracy: 0.0991\n",
      "Epoch 224/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 10.1137 - accuracy: 0.0973\n",
      "Epoch 225/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 48.7973 - accuracy: 0.0866\n",
      "Epoch 226/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 16.0803 - accuracy: 0.0985\n",
      "Epoch 227/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 20.3668 - accuracy: 0.1104\n",
      "Epoch 228/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 12.7540 - accuracy: 0.1169\n",
      "Epoch 229/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 22.9788 - accuracy: 0.1025\n",
      "Epoch 230/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 33.9550 - accuracy: 0.0803\n",
      "Epoch 231/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 17.2468 - accuracy: 0.1025\n",
      "Epoch 232/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 8.5187 - accuracy: 0.1126\n",
      "Epoch 233/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 17.3269 - accuracy: 0.1072\n",
      "Epoch 234/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 7.6383 - accuracy: 0.1073\n",
      "Epoch 235/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 57.8061 - accuracy: 0.0963\n",
      "Epoch 236/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 24.1430 - accuracy: 0.1012\n",
      "Epoch 237/580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 1ms/step - loss: 15.5499 - accuracy: 0.1060\n",
      "Epoch 238/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 10.8994 - accuracy: 0.1084\n",
      "Epoch 239/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 68.4571 - accuracy: 0.1039\n",
      "Epoch 240/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 21.4699 - accuracy: 0.1028\n",
      "Epoch 241/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 15.4150 - accuracy: 0.1057\n",
      "Epoch 242/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 10.2871 - accuracy: 0.1038\n",
      "Epoch 243/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 42.9146 - accuracy: 0.0986\n",
      "Epoch 244/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 23.1142 - accuracy: 0.1172\n",
      "Epoch 245/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.2951 - accuracy: 0.1133\n",
      "Epoch 246/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 10.8645 - accuracy: 0.1099\n",
      "Epoch 247/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 52.5229 - accuracy: 0.1163\n",
      "Epoch 248/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 36.2104 - accuracy: 0.0925\n",
      "Epoch 249/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 19.5793 - accuracy: 0.1075\n",
      "Epoch 250/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 46.8197 - accuracy: 0.1072\n",
      "Epoch 251/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.5148 - accuracy: 0.1036\n",
      "Epoch 252/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 11.1267 - accuracy: 0.1068\n",
      "Epoch 253/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 65.1652 - accuracy: 0.0970\n",
      "Epoch 254/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 30.7211 - accuracy: 0.0953\n",
      "Epoch 255/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 17.0949 - accuracy: 0.0941\n",
      "Epoch 256/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 11.3687 - accuracy: 0.1022\n",
      "Epoch 257/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.2151 - accuracy: 0.0987\n",
      "Epoch 258/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.3862 - accuracy: 0.0969\n",
      "Epoch 259/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 13.4983 - accuracy: 0.1098\n",
      "Epoch 260/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 9.7912 - accuracy: 0.1083\n",
      "Epoch 261/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 6.7272 - accuracy: 0.1096\n",
      "Epoch 262/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 70.9194 - accuracy: 0.0967\n",
      "Epoch 263/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 19.6956 - accuracy: 0.1053\n",
      "Epoch 264/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 21.9584 - accuracy: 0.1141\n",
      "Epoch 265/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 16.0763 - accuracy: 0.1021\n",
      "Epoch 266/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 41.4409 - accuracy: 0.1024\n",
      "Epoch 267/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 26.4888 - accuracy: 0.0989\n",
      "Epoch 268/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 12.5207 - accuracy: 0.1099\n",
      "Epoch 269/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 6.7524 - accuracy: 0.1072\n",
      "Epoch 270/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 43.7581 - accuracy: 0.0976\n",
      "Epoch 271/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 19.5292 - accuracy: 0.0976\n",
      "Epoch 272/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 15.9743 - accuracy: 0.1039\n",
      "Epoch 273/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 22.5500 - accuracy: 0.0924\n",
      "Epoch 274/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 12.2444 - accuracy: 0.0933\n",
      "Epoch 275/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 60.3471 - accuracy: 0.0819\n",
      "Epoch 276/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 24.3527 - accuracy: 0.0945\n",
      "Epoch 277/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.0751 - accuracy: 0.1015\n",
      "Epoch 278/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 17.4007 - accuracy: 0.1025\n",
      "Epoch 279/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 26.1882 - accuracy: 0.0984\n",
      "Epoch 280/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 21.1442 - accuracy: 0.1028\n",
      "Epoch 281/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 17.0975 - accuracy: 0.1029\n",
      "Epoch 282/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 12.9319 - accuracy: 0.1061\n",
      "Epoch 283/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 46.0882 - accuracy: 0.1073\n",
      "Epoch 284/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 27.7359 - accuracy: 0.0967\n",
      "Epoch 285/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 68.1685 - accuracy: 0.0958\n",
      "Epoch 286/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.4611 - accuracy: 0.1001\n",
      "Epoch 287/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.7741 - accuracy: 0.1014\n",
      "Epoch 288/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 8.9563 - accuracy: 0.1039\n",
      "Epoch 289/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 39.4242 - accuracy: 0.1005\n",
      "Epoch 290/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.5104 - accuracy: 0.1052\n",
      "Epoch 291/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 9.5120 - accuracy: 0.1102\n",
      "Epoch 292/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 33.9310 - accuracy: 0.0951\n",
      "Epoch 293/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 8.8190 - accuracy: 0.1020\n",
      "Epoch 294/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 23.7212 - accuracy: 0.0983\n",
      "Epoch 295/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 30.2135 - accuracy: 0.0911\n",
      "Epoch 296/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.2165 - accuracy: 0.0992\n",
      "Epoch 297/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 13.1850 - accuracy: 0.1073\n",
      "Epoch 298/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 42.3273 - accuracy: 0.0982\n",
      "Epoch 299/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 19.7010 - accuracy: 0.0975\n",
      "Epoch 300/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 17.8559 - accuracy: 0.0985\n",
      "Epoch 301/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 13.1487 - accuracy: 0.1056\n",
      "Epoch 302/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 30.8389 - accuracy: 0.0992\n",
      "Epoch 303/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 21.7057 - accuracy: 0.0987\n",
      "Epoch 304/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.0228 - accuracy: 0.0954\n",
      "Epoch 305/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 22.1398 - accuracy: 0.0987\n",
      "Epoch 306/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.5628 - accuracy: 0.0967\n",
      "Epoch 307/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 33.7023 - accuracy: 0.0966\n",
      "Epoch 308/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 50.7167 - accuracy: 0.0992\n",
      "Epoch 309/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 28.9360 - accuracy: 0.1005\n",
      "Epoch 310/580\n",
      "453/453 [==============================] - 1s 1ms/step - loss: 65.4898 - accuracy: 0.0903\n",
      "Epoch 311/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 42.3511 - accuracy: 0.0967\n",
      "Epoch 312/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.8481 - accuracy: 0.1190\n",
      "Epoch 313/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 115.2250 - accuracy: 0.1159\n",
      "Epoch 314/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 53.1532 - accuracy: 0.0934\n",
      "Epoch 315/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 40.4352 - accuracy: 0.1009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 30.1623 - accuracy: 0.1074\n",
      "Epoch 317/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 59.2622 - accuracy: 0.1109\n",
      "Epoch 318/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 38.7186 - accuracy: 0.0988\n",
      "Epoch 319/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 24.3890 - accuracy: 0.1052\n",
      "Epoch 320/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 71.1952 - accuracy: 0.1043\n",
      "Epoch 321/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 61.8470 - accuracy: 0.0954\n",
      "Epoch 322/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 34.2319 - accuracy: 0.0987\n",
      "Epoch 323/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 21.4557 - accuracy: 0.1034\n",
      "Epoch 324/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.7024 - accuracy: 0.1045\n",
      "Epoch 325/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 123.8372 - accuracy: 0.0934\n",
      "Epoch 326/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 47.7080 - accuracy: 0.1014\n",
      "Epoch 327/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 31.9563 - accuracy: 0.1054\n",
      "Epoch 328/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 18.0667 - accuracy: 0.1106\n",
      "Epoch 329/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 13.2460 - accuracy: 0.1135\n",
      "Epoch 330/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 111.3247 - accuracy: 0.0903\n",
      "Epoch 331/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 42.7029 - accuracy: 0.0950\n",
      "Epoch 332/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 28.9785 - accuracy: 0.0996\n",
      "Epoch 333/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 21.2008 - accuracy: 0.1019\n",
      "Epoch 334/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 122.4972 - accuracy: 0.0928\n",
      "Epoch 335/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 38.3429 - accuracy: 0.1077\n",
      "Epoch 336/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 20.6286 - accuracy: 0.1293\n",
      "Epoch 337/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 62.0350 - accuracy: 0.1226\n",
      "Epoch 338/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 16.3850 - accuracy: 0.1067\n",
      "Epoch 339/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 63.9763 - accuracy: 0.1069\n",
      "Epoch 340/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 59.2675 - accuracy: 0.0932\n",
      "Epoch 341/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 37.4725 - accuracy: 0.0985\n",
      "Epoch 342/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 24.6125 - accuracy: 0.1093\n",
      "Epoch 343/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.1733 - accuracy: 0.1076\n",
      "Epoch 344/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 110.0318 - accuracy: 0.1048\n",
      "Epoch 345/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 41.8479 - accuracy: 0.1026\n",
      "Epoch 346/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 24.7187 - accuracy: 0.1083\n",
      "Epoch 347/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 91.1795 - accuracy: 0.0961\n",
      "Epoch 348/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 46.5638 - accuracy: 0.0956\n",
      "Epoch 349/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 24.4555 - accuracy: 0.1020\n",
      "Epoch 350/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 39.5693 - accuracy: 0.0951\n",
      "Epoch 351/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 23.2925 - accuracy: 0.0919\n",
      "Epoch 352/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.5019 - accuracy: 0.0963\n",
      "Epoch 353/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 104.1904 - accuracy: 0.0911\n",
      "Epoch 354/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 50.8184 - accuracy: 0.0913\n",
      "Epoch 355/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 33.4443 - accuracy: 0.0943\n",
      "Epoch 356/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 107.8292 - accuracy: 0.0960\n",
      "Epoch 357/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 67.5484 - accuracy: 0.0942\n",
      "Epoch 358/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 21.1125 - accuracy: 0.1033\n",
      "Epoch 359/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 41.4152 - accuracy: 0.0976\n",
      "Epoch 360/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 55.5082 - accuracy: 0.0903\n",
      "Epoch 361/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 38.0485 - accuracy: 0.0956\n",
      "Epoch 362/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 164.9741 - accuracy: 0.0954\n",
      "Epoch 363/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 32.3157 - accuracy: 0.0943\n",
      "Epoch 364/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 54.1228 - accuracy: 0.1092\n",
      "Epoch 365/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 120.1901 - accuracy: 0.0901\n",
      "Epoch 366/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 42.8505 - accuracy: 0.0964\n",
      "Epoch 367/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 136.8288 - accuracy: 0.0907\n",
      "Epoch 368/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 84.6305 - accuracy: 0.0861\n",
      "Epoch 369/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 60.4467 - accuracy: 0.0923\n",
      "Epoch 370/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 43.6648 - accuracy: 0.0967\n",
      "Epoch 371/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 30.0304 - accuracy: 0.0978\n",
      "Epoch 372/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 18.7929 - accuracy: 0.0990\n",
      "Epoch 373/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 10.8797 - accuracy: 0.1052\n",
      "Epoch 374/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 116.5786 - accuracy: 0.0962\n",
      "Epoch 375/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 56.6528 - accuracy: 0.0950\n",
      "Epoch 376/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 37.4319 - accuracy: 0.0988\n",
      "Epoch 377/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.6118 - accuracy: 0.0974\n",
      "Epoch 378/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 59.8129 - accuracy: 0.0945\n",
      "Epoch 379/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 99.5891 - accuracy: 0.0945\n",
      "Epoch 380/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 78.2413 - accuracy: 0.0845\n",
      "Epoch 381/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 42.2536 - accuracy: 0.0946\n",
      "Epoch 382/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 33.7271 - accuracy: 0.0962\n",
      "Epoch 383/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 138.2021 - accuracy: 0.1084\n",
      "Epoch 384/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 81.3253 - accuracy: 0.0849\n",
      "Epoch 385/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 25.2417 - accuracy: 0.1075\n",
      "Epoch 386/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 84.2353 - accuracy: 0.1062\n",
      "Epoch 387/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 79.2301 - accuracy: 0.0921\n",
      "Epoch 388/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 54.5587 - accuracy: 0.0911\n",
      "Epoch 389/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 37.1059 - accuracy: 0.0967\n",
      "Epoch 390/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 50.6501 - accuracy: 0.0950\n",
      "Epoch 391/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 27.6943 - accuracy: 0.0949\n",
      "Epoch 392/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 18.5914 - accuracy: 0.0963\n",
      "Epoch 393/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 155.8641 - accuracy: 0.0892\n",
      "Epoch 394/580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 1ms/step - loss: 73.3839 - accuracy: 0.0842\n",
      "Epoch 395/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 291.9670 - accuracy: 0.0881\n",
      "Epoch 396/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 109.9910 - accuracy: 0.0831\n",
      "Epoch 397/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 70.1483 - accuracy: 0.0923\n",
      "Epoch 398/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 33.1910 - accuracy: 0.1000\n",
      "Epoch 399/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 239.0868 - accuracy: 0.0912\n",
      "Epoch 400/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 76.3516 - accuracy: 0.0925\n",
      "Epoch 401/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 52.9179 - accuracy: 0.0940\n",
      "Epoch 402/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 36.0230 - accuracy: 0.0936\n",
      "Epoch 403/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 21.3503 - accuracy: 0.0976\n",
      "Epoch 404/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 65.4836 - accuracy: 0.0904\n",
      "Epoch 405/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 23.4102 - accuracy: 0.0924\n",
      "Epoch 406/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 14.7967 - accuracy: 0.0969\n",
      "Epoch 407/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 133.0604 - accuracy: 0.0918\n",
      "Epoch 408/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 79.5782 - accuracy: 0.0895\n",
      "Epoch 409/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 56.6331 - accuracy: 0.0934\n",
      "Epoch 410/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 136.9482 - accuracy: 0.0892\n",
      "Epoch 411/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 102.9136 - accuracy: 0.0876\n",
      "Epoch 412/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 69.1463 - accuracy: 0.0910\n",
      "Epoch 413/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 50.9956 - accuracy: 0.0939\n",
      "Epoch 414/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 33.1547 - accuracy: 0.0918\n",
      "Epoch 415/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 20.6084 - accuracy: 0.0958\n",
      "Epoch 416/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 72.5468 - accuracy: 0.0890\n",
      "Epoch 417/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 78.3577 - accuracy: 0.0885\n",
      "Epoch 418/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 105.9313 - accuracy: 0.0838\n",
      "Epoch 419/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 72.8969 - accuracy: 0.0850\n",
      "Epoch 420/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 69.8912 - accuracy: 0.0893\n",
      "Epoch 421/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 45.2903 - accuracy: 0.0876\n",
      "Epoch 422/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 70.6898 - accuracy: 0.0889\n",
      "Epoch 423/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 85.6266 - accuracy: 0.0823\n",
      "Epoch 424/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 70.0451 - accuracy: 0.0882\n",
      "Epoch 425/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 41.2837 - accuracy: 0.0890\n",
      "Epoch 426/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 27.1087 - accuracy: 0.0905\n",
      "Epoch 427/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 164.5499 - accuracy: 0.0832\n",
      "Epoch 428/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 81.0050 - accuracy: 0.0811\n",
      "Epoch 429/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 63.9518 - accuracy: 0.0849\n",
      "Epoch 430/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 92.6609 - accuracy: 0.0852\n",
      "Epoch 431/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 60.5673 - accuracy: 0.0821\n",
      "Epoch 432/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 41.1517 - accuracy: 0.0859\n",
      "Epoch 433/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 30.3104 - accuracy: 0.0917\n",
      "Epoch 434/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 166.9792 - accuracy: 0.0768\n",
      "Epoch 435/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 84.9460 - accuracy: 0.0801\n",
      "Epoch 436/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 63.7213 - accuracy: 0.0849\n",
      "Epoch 437/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 48.2760 - accuracy: 0.0883\n",
      "Epoch 438/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 34.1658 - accuracy: 0.0925\n",
      "Epoch 439/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 137.1944 - accuracy: 0.0869\n",
      "Epoch 440/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 68.6124 - accuracy: 0.0862\n",
      "Epoch 441/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 42.8541 - accuracy: 0.0890\n",
      "Epoch 442/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 28.2497 - accuracy: 0.0919\n",
      "Epoch 443/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 17.4755 - accuracy: 0.0926\n",
      "Epoch 444/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 182.8149 - accuracy: 0.0924\n",
      "Epoch 445/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 86.5304 - accuracy: 0.0880\n",
      "Epoch 446/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 121.0022 - accuracy: 0.0877\n",
      "Epoch 447/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 54.2855 - accuracy: 0.0923\n",
      "Epoch 448/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 30.2725 - accuracy: 0.0965\n",
      "Epoch 449/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 22.9811 - accuracy: 0.1045\n",
      "Epoch 450/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 123.5668 - accuracy: 0.0894\n",
      "Epoch 451/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 222.8323 - accuracy: 0.0902\n",
      "Epoch 452/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 178.9215 - accuracy: 0.0927\n",
      "Epoch 453/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 97.2165 - accuracy: 0.0881\n",
      "Epoch 454/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 82.0851 - accuracy: 0.0921\n",
      "Epoch 455/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 51.1985 - accuracy: 0.0934\n",
      "Epoch 456/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 30.6832 - accuracy: 0.0986\n",
      "Epoch 457/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 48.5624 - accuracy: 0.1007\n",
      "Epoch 458/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 199.9606 - accuracy: 0.0872\n",
      "Epoch 459/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 112.4936 - accuracy: 0.0901\n",
      "Epoch 460/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 58.5784 - accuracy: 0.0920\n",
      "Epoch 461/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 40.2820 - accuracy: 0.0943\n",
      "Epoch 462/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 43.1049 - accuracy: 0.0941\n",
      "Epoch 463/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 22.3429 - accuracy: 0.0953\n",
      "Epoch 464/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 75.6825 - accuracy: 0.0957\n",
      "Epoch 465/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 91.0526 - accuracy: 0.0900\n",
      "Epoch 466/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 110.5170 - accuracy: 0.0879\n",
      "Epoch 467/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 65.4537 - accuracy: 0.0974\n",
      "Epoch 468/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 134.6779 - accuracy: 0.0912\n",
      "Epoch 469/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 92.3466 - accuracy: 0.0895\n",
      "Epoch 470/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 53.1191 - accuracy: 0.0932\n",
      "Epoch 471/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 48.1833 - accuracy: 0.0967\n",
      "Epoch 472/580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 1ms/step - loss: 98.2341 - accuracy: 0.0963\n",
      "Epoch 473/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 48.8450 - accuracy: 0.0912\n",
      "Epoch 474/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 53.9424 - accuracy: 0.0951\n",
      "Epoch 475/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 253.6872 - accuracy: 0.0954\n",
      "Epoch 476/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 139.5042 - accuracy: 0.0844\n",
      "Epoch 477/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 96.8735 - accuracy: 0.0905\n",
      "Epoch 478/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 125.3002 - accuracy: 0.0916\n",
      "Epoch 479/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 98.0958 - accuracy: 0.0930\n",
      "Epoch 480/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 45.3793 - accuracy: 0.0922\n",
      "Epoch 481/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 145.8061 - accuracy: 0.0910\n",
      "Epoch 482/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 84.2088 - accuracy: 0.0901\n",
      "Epoch 483/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 52.1089 - accuracy: 0.0954\n",
      "Epoch 484/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 58.6820 - accuracy: 0.0977\n",
      "Epoch 485/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 156.6059 - accuracy: 0.0898\n",
      "Epoch 486/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 61.5310 - accuracy: 0.0900\n",
      "Epoch 487/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 346.0919 - accuracy: 0.0919\n",
      "Epoch 488/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 114.1053 - accuracy: 0.0887\n",
      "Epoch 489/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 121.1509 - accuracy: 0.0928\n",
      "Epoch 490/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 57.3433 - accuracy: 0.0953\n",
      "Epoch 491/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 34.2500 - accuracy: 0.0999\n",
      "Epoch 492/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 42.8959 - accuracy: 0.0963\n",
      "Epoch 493/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 148.7706 - accuracy: 0.0902\n",
      "Epoch 494/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 109.0188 - accuracy: 0.0915\n",
      "Epoch 495/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 128.7870 - accuracy: 0.0989\n",
      "Epoch 496/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 46.8930 - accuracy: 0.0972\n",
      "Epoch 497/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 138.9860 - accuracy: 0.0887\n",
      "Epoch 498/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 59.2572 - accuracy: 0.0941\n",
      "Epoch 499/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 216.7052 - accuracy: 0.0881\n",
      "Epoch 500/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 69.8475 - accuracy: 0.0891\n",
      "Epoch 501/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 41.1296 - accuracy: 0.0933\n",
      "Epoch 502/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 23.7504 - accuracy: 0.0990\n",
      "Epoch 503/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 135.3473 - accuracy: 0.0852\n",
      "Epoch 504/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 75.8648 - accuracy: 0.0922\n",
      "Epoch 505/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 62.7352 - accuracy: 0.0942\n",
      "Epoch 506/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 49.9341 - accuracy: 0.0939\n",
      "Epoch 507/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 164.0476 - accuracy: 0.0885\n",
      "Epoch 508/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 82.2354 - accuracy: 0.1068\n",
      "Epoch 509/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 44.9507 - accuracy: 0.1011\n",
      "Epoch 510/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 36.5667 - accuracy: 0.0983\n",
      "Epoch 511/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 282.6358 - accuracy: 0.0881\n",
      "Epoch 512/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 107.5487 - accuracy: 0.0936\n",
      "Epoch 513/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 116.4114 - accuracy: 0.0887\n",
      "Epoch 514/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 84.4790 - accuracy: 0.0870\n",
      "Epoch 515/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 49.4164 - accuracy: 0.0932\n",
      "Epoch 516/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 89.5888 - accuracy: 0.0954\n",
      "Epoch 517/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 96.2231 - accuracy: 0.0849\n",
      "Epoch 518/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 52.9335 - accuracy: 0.0925\n",
      "Epoch 519/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 94.2087 - accuracy: 0.0933\n",
      "Epoch 520/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 91.4960 - accuracy: 0.0936\n",
      "Epoch 521/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 141.6987 - accuracy: 0.0854\n",
      "Epoch 522/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 88.6664 - accuracy: 0.0996\n",
      "Epoch 523/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 74.4297 - accuracy: 0.1005\n",
      "Epoch 524/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 152.4243 - accuracy: 0.0998\n",
      "Epoch 525/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 116.5866 - accuracy: 0.0841\n",
      "Epoch 526/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 83.0592 - accuracy: 0.0948\n",
      "Epoch 527/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 123.3947 - accuracy: 0.0927\n",
      "Epoch 528/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 105.2900 - accuracy: 0.0900\n",
      "Epoch 529/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 95.5070 - accuracy: 0.0863\n",
      "Epoch 530/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 52.2442 - accuracy: 0.0939\n",
      "Epoch 531/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 77.5224 - accuracy: 0.0963\n",
      "Epoch 532/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 187.5209 - accuracy: 0.0860\n",
      "Epoch 533/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 119.4198 - accuracy: 0.0790\n",
      "Epoch 534/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 41.3969 - accuracy: 0.0899\n",
      "Epoch 535/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 146.5839 - accuracy: 0.0805\n",
      "Epoch 536/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 113.2259 - accuracy: 0.0857\n",
      "Epoch 537/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 134.6502 - accuracy: 0.0869\n",
      "Epoch 538/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 50.2302 - accuracy: 0.0885\n",
      "Epoch 539/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 53.6696 - accuracy: 0.0887\n",
      "Epoch 540/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 166.1833 - accuracy: 0.0985\n",
      "Epoch 541/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 145.1263 - accuracy: 0.0802\n",
      "Epoch 542/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 107.7009 - accuracy: 0.0772\n",
      "Epoch 543/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 54.3155 - accuracy: 0.0870\n",
      "Epoch 544/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 46.5114 - accuracy: 0.0854\n",
      "Epoch 545/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 23.4677 - accuracy: 0.0922\n",
      "Epoch 546/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 152.2872 - accuracy: 0.0767\n",
      "Epoch 547/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 42.9186 - accuracy: 0.0870\n",
      "Epoch 548/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 43.4396 - accuracy: 0.0809\n",
      "Epoch 549/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 81.4403 - accuracy: 0.0857\n",
      "Epoch 550/580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 0s 1ms/step - loss: 108.2066 - accuracy: 0.0729\n",
      "Epoch 551/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 92.5143 - accuracy: 0.0867\n",
      "Epoch 552/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 70.1447 - accuracy: 0.0777\n",
      "Epoch 553/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 38.2691 - accuracy: 0.0830\n",
      "Epoch 554/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 30.5475 - accuracy: 0.0950\n",
      "Epoch 555/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 23.7632 - accuracy: 0.0873\n",
      "Epoch 556/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 174.6390 - accuracy: 0.0796\n",
      "Epoch 557/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 62.0675 - accuracy: 0.0807\n",
      "Epoch 558/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 68.0980 - accuracy: 0.0822\n",
      "Epoch 559/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 21.1907 - accuracy: 0.0874\n",
      "Epoch 560/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 72.7730 - accuracy: 0.0777\n",
      "Epoch 561/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 29.0072 - accuracy: 0.0876\n",
      "Epoch 562/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 48.8181 - accuracy: 0.0869\n",
      "Epoch 563/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 117.8016 - accuracy: 0.0829\n",
      "Epoch 564/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 94.5358 - accuracy: 0.0807\n",
      "Epoch 565/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 168.1172 - accuracy: 0.0933\n",
      "Epoch 566/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 40.1202 - accuracy: 0.0904\n",
      "Epoch 567/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 175.2600 - accuracy: 0.0823\n",
      "Epoch 568/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 104.0878 - accuracy: 0.0791\n",
      "Epoch 569/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 76.8597 - accuracy: 0.0828\n",
      "Epoch 570/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 64.7616 - accuracy: 0.0946\n",
      "Epoch 571/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 149.5340 - accuracy: 0.0729\n",
      "Epoch 572/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 73.1976 - accuracy: 0.0847\n",
      "Epoch 573/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 27.0453 - accuracy: 0.0938\n",
      "Epoch 574/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 32.3031 - accuracy: 0.0945\n",
      "Epoch 575/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 217.9102 - accuracy: 0.0950\n",
      "Epoch 576/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 135.4037 - accuracy: 0.0774\n",
      "Epoch 577/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 201.6599 - accuracy: 0.0872\n",
      "Epoch 578/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 101.0469 - accuracy: 0.0846\n",
      "Epoch 579/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 94.5116 - accuracy: 0.0733\n",
      "Epoch 580/580\n",
      "453/453 [==============================] - 0s 1ms/step - loss: 51.5539 - accuracy: 0.0857\n",
      "151/151 [==============================] - 0s 521us/step\n",
      "Best Threshold: 0.03700000047683716 with G-Mean: 0.246\n",
      "FPR: 0.0636, TPR: 0.0646\n",
      "151/151 - 0s - loss: 71.7518 - accuracy: 0.0820 - 172ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the function using each of the target questions as an argument\n",
    "NN_Results = {}\n",
    "for target in targets:\n",
    "    NN_Results[target] = NN_Classifier(target,dataset_dictionary[target]['X'],dataset_dictionary[target]['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "w7GTegwC8bAx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "w7GTegwC8bAx",
    "outputId": "36eef2e8-29e1-4466-bd34-56caeb3f5e3b"
   },
   "outputs": [],
   "source": [
    "# create a results file \n",
    "NN_results = pd.DataFrame(NN_Results).to_csv('Results/results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9khhEhJWfgpt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9khhEhJWfgpt",
    "outputId": "fe3861da-a966-4bc9-b0aa-d66725739a65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P10_8_1': 0.03970000147819519,\n",
       " 'P10_8_2': 0.0348999984562397,\n",
       " 'P10_8_3': 0.010499999858438969,\n",
       " 'P10_8_4': 0.04399999976158142,\n",
       " 'P10_8_5': 0.0406000018119812,\n",
       " 'P10_8_6': 0.04360000044107437,\n",
       " 'P10_8_7': 0.03830000013113022,\n",
       " 'P10_8_8': 0.05480000004172325,\n",
       " 'P10_8_9': 0.0340999998152256,\n",
       " 'P10_8_10': 0.02810000069439411,\n",
       " 'P10_8_11': 0.03700000047683716}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(NN_Results).loc['Threshold'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pxx6xETgk5m2",
   "metadata": {
    "id": "pxx6xETgk5m2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P10_8_1</th>\n",
       "      <th>P10_8_2</th>\n",
       "      <th>P10_8_3</th>\n",
       "      <th>P10_8_4</th>\n",
       "      <th>P10_8_5</th>\n",
       "      <th>P10_8_6</th>\n",
       "      <th>P10_8_7</th>\n",
       "      <th>P10_8_8</th>\n",
       "      <th>P10_8_9</th>\n",
       "      <th>P10_8_10</th>\n",
       "      <th>P10_8_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Threshold</th>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0436</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predictions</th>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>[1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <td>[[216, 131], [1952, 2532]]</td>\n",
       "      <td>[[332, 172], [2110, 2217]]</td>\n",
       "      <td>[[22, 26], [1749, 3034]]</td>\n",
       "      <td>[[204, 98], [2093, 2436]]</td>\n",
       "      <td>[[291, 140], [2049, 2351]]</td>\n",
       "      <td>[[108, 79], [1867, 2777]]</td>\n",
       "      <td>[[245, 145], [2097, 2344]]</td>\n",
       "      <td>[[178, 9], [4429, 215]]</td>\n",
       "      <td>[[269, 196], [1846, 2520]]</td>\n",
       "      <td>[[56, 5], [4009, 761]]</td>\n",
       "      <td>[[103, 7], [4416, 305]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy Score</th>\n",
       "      <td>0.194577</td>\n",
       "      <td>0.223349</td>\n",
       "      <td>0.501759</td>\n",
       "      <td>0.13165</td>\n",
       "      <td>0.184641</td>\n",
       "      <td>0.352929</td>\n",
       "      <td>0.192507</td>\n",
       "      <td>0.07928</td>\n",
       "      <td>0.245084</td>\n",
       "      <td>0.16332</td>\n",
       "      <td>0.081971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classification Report</th>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 P10_8_1  \\\n",
       "Threshold                                                         0.0397   \n",
       "Predictions            [0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, ...   \n",
       "Confusion Matrix                              [[216, 131], [1952, 2532]]   \n",
       "Accuracy Score                                                  0.194577   \n",
       "Classification Report                precision    recall  f1-score   ...   \n",
       "\n",
       "                                                                 P10_8_2  \\\n",
       "Threshold                                                         0.0349   \n",
       "Predictions            [1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, ...   \n",
       "Confusion Matrix                              [[332, 172], [2110, 2217]]   \n",
       "Accuracy Score                                                  0.223349   \n",
       "Classification Report                precision    recall  f1-score   ...   \n",
       "\n",
       "                                                                 P10_8_3  \\\n",
       "Threshold                                                         0.0105   \n",
       "Predictions            [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, ...   \n",
       "Confusion Matrix                                [[22, 26], [1749, 3034]]   \n",
       "Accuracy Score                                                  0.501759   \n",
       "Classification Report                precision    recall  f1-score   ...   \n",
       "\n",
       "                                                                 P10_8_4  \\\n",
       "Threshold                                                          0.044   \n",
       "Predictions            [0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...   \n",
       "Confusion Matrix                               [[204, 98], [2093, 2436]]   \n",
       "Accuracy Score                                                   0.13165   \n",
       "Classification Report                precision    recall  f1-score   ...   \n",
       "\n",
       "                                                                 P10_8_5  \\\n",
       "Threshold                                                         0.0406   \n",
       "Predictions            [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, ...   \n",
       "Confusion Matrix                              [[291, 140], [2049, 2351]]   \n",
       "Accuracy Score                                                  0.184641   \n",
       "Classification Report                precision    recall  f1-score   ...   \n",
       "\n",
       "                                                                 P10_8_6  \\\n",
       "Threshold                                                         0.0436   \n",
       "Predictions            [1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, ...   \n",
       "Confusion Matrix                               [[108, 79], [1867, 2777]]   \n",
       "Accuracy Score                                                  0.352929   \n",
       "Classification Report                precision    recall  f1-score   ...   \n",
       "\n",
       "                                                                 P10_8_7  \\\n",
       "Threshold                                                         0.0383   \n",
       "Predictions            [1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, ...   \n",
       "Confusion Matrix                              [[245, 145], [2097, 2344]]   \n",
       "Accuracy Score                                                  0.192507   \n",
       "Classification Report                precision    recall  f1-score   ...   \n",
       "\n",
       "                                                                 P10_8_8  \\\n",
       "Threshold                                                         0.0548   \n",
       "Predictions            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "Confusion Matrix                                 [[178, 9], [4429, 215]]   \n",
       "Accuracy Score                                                   0.07928   \n",
       "Classification Report                precision    recall  f1-score   ...   \n",
       "\n",
       "                                                                 P10_8_9  \\\n",
       "Threshold                                                         0.0341   \n",
       "Predictions            [0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, ...   \n",
       "Confusion Matrix                              [[269, 196], [1846, 2520]]   \n",
       "Accuracy Score                                                  0.245084   \n",
       "Classification Report                precision    recall  f1-score   ...   \n",
       "\n",
       "                                                                P10_8_10  \\\n",
       "Threshold                                                         0.0281   \n",
       "Predictions            [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "Confusion Matrix                                  [[56, 5], [4009, 761]]   \n",
       "Accuracy Score                                                   0.16332   \n",
       "Classification Report                precision    recall  f1-score   ...   \n",
       "\n",
       "                                                                P10_8_11  \n",
       "Threshold                                                          0.037  \n",
       "Predictions            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "Confusion Matrix                                 [[103, 7], [4416, 305]]  \n",
       "Accuracy Score                                                  0.081971  \n",
       "Classification Report                precision    recall  f1-score   ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(NN_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e077508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
